{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iqh_G6doi_L6",
        "outputId": "aefe158d-6b7d-4947-f2cd-93715b95550c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "0J0zAlT0jTTK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEK0pndTjfSp",
        "outputId": "d68bd03d-ded0-414a-d156-b5fac0d82dff"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5iUseKGj2xZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Purpose of Text Preprocessing in NLP:\n",
        "Text preprocessing in Natural Language Processing (NLP) serves several crucial purposes:\n",
        "\n",
        "Noise Reduction: Cleaning and handling noisy data, such as HTML tags, special characters, and irrelevant information.\n",
        "\n",
        "Normalization: Ensuring consistent representation of text, like converting all letters to lowercase.\n",
        "\n",
        "Tokenization: Breaking down text into smaller units (tokens), like words or subwords.\n",
        "\n",
        "Stemming and Lemmatization: Reducing words to their base or root form for analysis.\n",
        "\n",
        "Removing Stop Words: Eliminating common words that don't contribute much to the overall meaning.\n",
        "\n",
        "Vectorization: Converting text into numerical representations suitable for machine learning models."
      ],
      "metadata": {
        "id": "FFzGSUorj4PR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Tokenization in NLP:\n",
        "Tokenization is the process of breaking down a text into words or subwords (tokens). It is a critical step in text processing as it forms the foundation for further analysis. Tokenization helps in understanding the structure of a sentence or document, enabling the application of various NLP techniques. In Python, the nltk library is commonly used for tokenization:"
      ],
      "metadata": {
        "id": "o5_dX48rj_kK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XFCK3r3wkIi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Tokenization and Feature extraction is an essential step in NLP.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA6t8GJTjvDE",
        "outputId": "30b6cdf3-cb42-4cfb-ca45-8aae93ba435d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'and', 'Feature', 'extraction', 'is', 'an', 'essential', 'step', 'in', 'NLP', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U0P4J07Tjxt6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Stemming vs. Lemmatization: Stemming: Reducing words to their base or root form by removing suffixes. It is a faster but less accurate method."
      ],
      "metadata": {
        "id": "dS_TC4b7kNwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "word = \"running\"\n",
        "stemmed_word = stemmer.stem(word)\n",
        "print(stemmed_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDLw_EeTkSu6",
        "outputId": "34521efa-5adc-48dd-b5d6-5f9d6cfe53b8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dy6y8UftkzAs",
        "outputId": "0d1967de-776f-4b25-b794-f258ad337bee"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization: Obtaining the base or dictionary form of a word (lemma). It's slower but more accurate than stemming."
      ],
      "metadata": {
        "id": "LNBtz0SWkpMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "word = \"running\"\n",
        "lemmatized_word = lemmatizer.lemmatize(word, pos='v')\n",
        "print(lemmatized_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em45xUrvkmmK",
        "outputId": "b5a29ab6-7884-47b7-bdd0-6af9d04425b1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose stemming when speed is crucial and a little loss of accuracy is acceptable. Opt for lemmatization when precision is more critical.\n",
        "\n",
        "4.Stop Words in Text Preprocessing: Stop words are common words like \"the,\" \"and,\" \"is\" that are often removed during text preprocessing. They don't carry much meaning and can be a source of noise in analysis. In Python, using the nltk library"
      ],
      "metadata": {
        "id": "UDUpORPGlHM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uvLQvujlaKU",
        "outputId": "5090227b-07ce-4fa0-b4f0-1d899d540bab"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_text = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(filtered_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCE9wAYBlOXz",
        "outputId": "666e4372-4642-40b1-a90e-7fa3544349a0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'Feature', 'extraction', 'essential', 'step', 'NLP', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Removing Punctuation: Removing punctuation is essential to ensure consistency and focus on the actual words. It helps in reducing the dimensionality of the data and simplifies analysis."
      ],
      "metadata": {
        "id": "w4CuCCjmlo1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "text = \"Text with punctuation!\"\n",
        "clean_text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "print(clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBrY5Ojble3L",
        "outputId": "f2ec2e5d-c588-43c8-9dee-89df137fd2ab"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text with punctuation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Lowercase Conversion: Converting text to lowercase is a common step to ensure uniformity. It helps in treating words with different cases as the same word, reducing complexity."
      ],
      "metadata": {
        "id": "mYFbmcDHmEDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a Sample Tex for NLP.\"\n",
        "lowercase_text = text.lower()\n",
        "print(lowercase_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBV4NGGRlnco",
        "outputId": "b786a5c9-68d2-449c-8570-8b061aaf1c44"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a sample tex for nlp.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Vectorization in NLP: Vectorization is the process of converting text data into numerical vectors. CountVectorizer is a popular technique that represents each document as a vector of word frequencies."
      ],
      "metadata": {
        "id": "foPK3nd0mOvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\"This is a sample document.\", \"Another example document.\"]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adLYgEkymLfT",
        "outputId": "ad7eb206-6dad-44c4-d39c-c5066c0a856b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 0 1 1 1]\n",
            " [1 1 1 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Normalization in NLP: Normalization involves transforming text data to a standard format. Techniques include lowercasing, stemming, lemmatization, and removing stop words."
      ],
      "metadata": {
        "id": "cO5YvX9OmeGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_text = [lemmatizer.lemmatize(word.lower()) for word in tokens if word.lower() not in stop_words]\n",
        "print(normalized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzxwsjd7mbQM",
        "outputId": "3caed147-20e5-4738-ed1e-6551a8da71ce"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tokenization', 'feature', 'extraction', 'essential', 'step', 'nlp', '.']\n"
          ]
        }
      ]
    }
  ]
}