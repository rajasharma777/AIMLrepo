{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f262de8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4850df18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>mainroad</th>\n",
       "      <th>guestroom</th>\n",
       "      <th>basement</th>\n",
       "      <th>hotwaterheating</th>\n",
       "      <th>airconditioning</th>\n",
       "      <th>parking</th>\n",
       "      <th>prefarea</th>\n",
       "      <th>furnishingstatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13300000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12250000</td>\n",
       "      <td>8960</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12250000</td>\n",
       "      <td>9960</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>semi-furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12215000</td>\n",
       "      <td>7500</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11410000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>1820000</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>unfurnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>1767150</td>\n",
       "      <td>2400</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>semi-furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>1750000</td>\n",
       "      <td>3620</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>unfurnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>1750000</td>\n",
       "      <td>2910</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>1750000</td>\n",
       "      <td>3850</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>unfurnished</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>545 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        price  area  bedrooms  bathrooms  stories mainroad guestroom basement  \\\n",
       "0    13300000  7420         4          2        3      yes        no       no   \n",
       "1    12250000  8960         4          4        4      yes        no       no   \n",
       "2    12250000  9960         3          2        2      yes        no      yes   \n",
       "3    12215000  7500         4          2        2      yes        no      yes   \n",
       "4    11410000  7420         4          1        2      yes       yes      yes   \n",
       "..        ...   ...       ...        ...      ...      ...       ...      ...   \n",
       "540   1820000  3000         2          1        1      yes        no      yes   \n",
       "541   1767150  2400         3          1        1       no        no       no   \n",
       "542   1750000  3620         2          1        1      yes        no       no   \n",
       "543   1750000  2910         3          1        1       no        no       no   \n",
       "544   1750000  3850         3          1        2      yes        no       no   \n",
       "\n",
       "    hotwaterheating airconditioning  parking prefarea furnishingstatus  \n",
       "0                no             yes        2      yes        furnished  \n",
       "1                no             yes        3       no        furnished  \n",
       "2                no              no        2      yes   semi-furnished  \n",
       "3                no             yes        3      yes        furnished  \n",
       "4                no             yes        2       no        furnished  \n",
       "..              ...             ...      ...      ...              ...  \n",
       "540              no              no        2       no      unfurnished  \n",
       "541              no              no        0       no   semi-furnished  \n",
       "542              no              no        0       no      unfurnished  \n",
       "543              no              no        0       no        furnished  \n",
       "544              no              no        0       no      unfurnished  \n",
       "\n",
       "[545 rows x 13 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('housing.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9998576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price               0\n",
       "area                0\n",
       "bedrooms            0\n",
       "bathrooms           0\n",
       "stories             0\n",
       "mainroad            0\n",
       "guestroom           0\n",
       "basement            0\n",
       "hotwaterheating     0\n",
       "airconditioning     0\n",
       "parking             0\n",
       "prefarea            0\n",
       "furnishingstatus    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#missing values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a016c89b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80547838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = data[['area', 'bedrooms', 'bathrooms', 'stories', 'parking']]  # Add other relevant numerical features\n",
    "y = data['price']\n",
    "\n",
    "# c. Split the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8494cdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Coefficients: [3.08902826e+02 1.51185146e+05 1.18536672e+06 4.95050002e+05\n",
      " 3.37542244e+05]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the Lasso model\n",
    "lasso_model = Lasso(alpha=0.01)  # You can tune the alpha parameter for regularization strength\n",
    "\n",
    "# Fit the model to the training data\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# b. Discuss the impact of L1 regularization on feature selection and coefficients\n",
    "# L1 regularization (Lasso) can force the coefficients of irrelevant features to be exactly 0,\n",
    "# effectively performing feature selection and simplifying the model.\n",
    "print(\"Lasso Coefficients:\", lasso_model.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab98e1bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ff57488b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAsso R-sq value :\t 0.5463945902723355\n",
      "Lasso Regression Metrics:\n",
      "Mean Absolute Error (MAE): 1127504.0986581189\n",
      "Mean Squared Error (MSE): 2292780407597.269\n",
      "Root Mean Squared Error (RMSE): 1514192.988887899\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "# Predict house prices on the test set\n",
    "lasso_predictions = lasso_model.predict(X_test)\n",
    "print('LAsso R-sq value :\\t',r2_score(y_test,lasso_predictions))\n",
    "# Calculate MAE, MSE, and RMSE\n",
    "lasso_mae = mean_absolute_error(y_test, lasso_predictions)\n",
    "lasso_mse = mean_squared_error(y_test, lasso_predictions)\n",
    "lasso_rmse = np.sqrt(lasso_mse)\n",
    "\n",
    "print(\"Lasso Regression Metrics:\")\n",
    "print(\"Mean Absolute Error (MAE):\", lasso_mae)\n",
    "print(\"Mean Squared Error (MSE):\", lasso_mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", lasso_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12f386c",
   "metadata": {},
   "source": [
    "Lasso regression's L1 regularization term penalizes large coefficients, encouraging the model to simplify by setting the coefficients of irrelevant features to zero. This feature selection property helps prevent overfitting by reducing the complexity of the model. It ensures that only the most relevant features are considered, effectively reducing the impact of irrelevant features on the model's predictions. This regularization technique is particularly useful when dealing with datasets containing both numerical and categorical features, allowing the model to\n",
    "handle feature selection and regularization simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d059bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge(alpha=0.001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(alpha=0.001)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Ridge(alpha=0.001)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_model = Ridge(alpha=0.01)\n",
    "ridge_model.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7dd7d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Explain how L2 regularization in Ridge regression differs from L1 regularization in Lasso.\n",
    "# L2 regularization (Ridge) penalizes the sum of squared coefficients (Euclidean norm), \n",
    "# while L1 regularization (Lasso) penalizes the sum of absolute values of coefficients (Manhattan norm).\n",
    "# The key difference is that L2 regularization does not force coefficients to be exactly zero, \n",
    "# but it discourages large coefficients by adding their squared values to the cost function. \n",
    "# This tends to distribute the impact of irrelevant features more evenly across all features \n",
    "# (keeping all features in the model) compared to Lasso, which may lead to sparse solutions (some coefficients being exactly zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cd429833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge R-sq value :\t 0.5464063262338164\n",
      "Ridge Regression Metrics:\n",
      "Mean Absolute Error (MAE): 1127483.3062465736\n",
      "Mean Squared Error (MSE): 2292721087355.555\n",
      "Root Mean Squared Error (RMSE): 1514173.4006894836\n"
     ]
    }
   ],
   "source": [
    "# Predict house prices on the test set\n",
    "ridge_predictions = ridge_model.predict(X_test)\n",
    "\n",
    "print('ridge R-sq value :\\t',r2_score(y_test,ridge_predictions))\n",
    "# Calculate MAE, MSE, and RMSE\n",
    "ridge_mae = mean_absolute_error(y_test, ridge_predictions)\n",
    "ridge_mse = mean_squared_error(y_test, ridge_predictions)\n",
    "ridge_rmse = np.sqrt(ridge_mse)\n",
    "\n",
    "print(\"Ridge Regression Metrics:\")\n",
    "print(\"Mean Absolute Error (MAE):\", ridge_mae)\n",
    "print(\"Mean Squared Error (MSE):\", ridge_mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", ridge_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a454d2d",
   "metadata": {},
   "source": [
    "b. Discuss the benefits of Ridge regression in handling multicollinearity among features and its impact on the model's coefficients:\n",
    "\n",
    "Ridge regression is particularly useful when dealing with multicollinearity, a situation where independent variables in a regression model are highly correlated. In the presence of multicollinearity, ordinary least squares (OLS) estimates can be unstable, leading to unreliable and highly sensitive coefficients. Ridge regression addresses this issue by adding the L2 regularization term, which discourages large coefficients.\n",
    "\n",
    "The L2 regularization term in Ridge regression penalizes the sum of squared coefficients. When features are highly correlated, Ridge regression works to distribute the impact of these correlated features more evenly across them, preventing any single feature from dominating the model. By doing so, Ridge regression helps stabilize the coefficient estimates, making them less sensitive to small changes in the data. This, in turn, leads to a more reliable and interpretable model, especially when dealing with datasets where multicollinearity is a concern. Additionally, Ridge regression can also help prevent overfitting in situations where there are many predictors relative to the number of observations, further enhancing the model's generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2435ff23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression Metrics:\n",
      "Mean Absolute Error (MAE): 1127504.0986581189\n",
      "Mean Squared Error (MSE): 2292780407597.269\n",
      "Root Mean Squared Error (RMSE): 1514192.988887899\n",
      "\n",
      "Ridge Regression Metrics:\n",
      "Mean Absolute Error (MAE): 1127483.3062465736\n",
      "Mean Squared Error (MSE): 2292721087355.555\n",
      "Root Mean Squared Error (RMSE): 1514173.4006894836\n"
     ]
    }
   ],
   "source": [
    "print(\"Lasso Regression Metrics:\")\n",
    "print(\"Mean Absolute Error (MAE):\", lasso_mae)\n",
    "print(\"Mean Squared Error (MSE):\", lasso_mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", lasso_rmse)\n",
    "\n",
    "print(\"\\nRidge Regression Metrics:\")\n",
    "print(\"Mean Absolute Error (MAE):\", ridge_mae)\n",
    "print(\"Mean Squared Error (MSE):\", ridge_mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", ridge_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9ec69e",
   "metadata": {},
   "source": [
    ". Discuss when it is preferable to use Lasso, Ridge, or plain linear regression:\n",
    "\n",
    "Plain Linear Regression: Use when you assume that all features are relevant and there is no issue of multicollinearity.\n",
    "\n",
    "Lasso Regression: Use when you suspect that there are irrelevant features in your dataset and you want to perform feature selection. Lasso is also useful when you have a large number of features and want to reduce the model's complexity.\n",
    "\n",
    "Ridge Regression: Use when you suspect there is multicollinearity in the dataset (correlation between predictors) as it handles multicollinearity well. It's also a good choice when you have a dataset with many predictors and you want to prevent overfitting by penalizing large coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d4c81351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Lasso Model: Lasso(alpha=100)\n",
      "Best Ridge Model: Ridge(alpha=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Hyperparameter tuning for Lasso\n",
    "lasso_params = {'alpha': [0.1, 1, 10, 100]}\n",
    "lasso_grid = GridSearchCV(Lasso(), param_grid=lasso_params, scoring='neg_mean_squared_error', cv=5)\n",
    "lasso_grid.fit(X_train, y_train)\n",
    "best_lasso = lasso_grid.best_estimator_\n",
    "print(\"Best Lasso Model:\", best_lasso)\n",
    "\n",
    "# Hyperparameter tuning for Ridge\n",
    "ridge_params = {'alpha': [0.1, 1, 10, 100]}\n",
    "ridge_grid = GridSearchCV(Ridge(), param_grid=ridge_params, scoring='neg_mean_squared_error', cv=5)\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "best_ridge = ridge_grid.best_estimator_\n",
    "print(\"Best Ridge Model:\", best_ridge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a769ede2",
   "metadata": {},
   "source": [
    "8. Model Improvement:\n",
    "\n",
    "a. Investigate feature engineering or data preprocessing techniques:\n",
    "\n",
    "Feature Scaling: Standardize or normalize numerical features to bring them to a similar scale, especially if you are using regularization techniques.\n",
    "\n",
    "Polynomial Features: Introduce interaction terms or polynomial features to capture nonlinear relationships between predictors.\n",
    "\n",
    "Handling Categorical Variables: Explore advanced techniques like target encoding or embedding layers for neural networks if you have categorical variables.\n",
    "\n",
    "Feature Selection: Conduct detailed feature analysis to identify the most influential features and focus on them, discarding irrelevant ones.\n",
    "\n",
    "Outlier Handling: Investigate outliers and apply appropriate techniques such as removing outliers or transforming skewed features.\n",
    "\n",
    "Data Imputation: If missing values exist, explore advanced imputation techniques like K-nearest neighbors imputation or predictive mean matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db70cbf",
   "metadata": {},
   "source": [
    "# 1. Initial Linear Regression Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42b66f0",
   "metadata": {},
   "source": [
    "a. Dataset and Variables:\n",
    "The dataset contains information about employees, with variables including experience (in years), education level (numeric representation), and the number of projects completed. The target variable is employee performance, measured using a numerical scale or score.\n",
    "\n",
    "b. Simple Linear Regression Model:\n",
    "The simple linear regression model predicts employee performance based on one predictor variable, for example, experience.\n",
    "\n",
    "c. Suitability of Linear Regression:\n",
    "Linear regression is suitable because it assumes a linear relationship between the predictors and the target variable. It's appropriate when predicting a numeric outcome (like performance scores) based on one or more predictor variables (like experience, education level, and projects completed).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f578f",
   "metadata": {},
   "source": [
    "2. Identifying Heteroscedasticity:\n",
    "a. Heteroscedasticity in Linear Regression:\n",
    "Heteroscedasticity refers to the situation where the\n",
    "variability of the residuals (the differences between observed and predicted values) is not constant across all \n",
    "levels of the predictor variables. In simpler terms, \n",
    "the spread of the residuals increases or decreases as the predicted values \n",
    "change.\n",
    "\n",
    "b. Methods for Diagnosing Heteroscedasticity:\n",
    "\n",
    "Residual Plot: Plotting residuals against predicted values can visually reveal patterns.\n",
    "Breusch-Pagan Test: A statistical test that formally tests for heteroscedasticity.\n",
    "Whiteâ€™s Test: Another statistical test for heteroscedasticity.\n",
    "    \n",
    "c. Diagnostic Results:\n",
    "\n",
    "After applying these methods to your model's residuals, you find evidence of heteroscedasticity, suggesting that the variability\n",
    "in employee performance prediction\n",
    "errors is not constant across different levels of predictor variables.    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdb993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Remedying Heteroscedasticity:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4420e5a6",
   "metadata": {},
   "source": [
    "a. Consequences of Heteroscedasticity:\n",
    "\n",
    "Heteroscedasticity can lead to inefficient parameter estimates and can affect the statistical tests for the regression coefficients. Standard errors might be biased,\n",
    "leading to incorrect conclusions about the significance of predictors.\n",
    "\n",
    "b. Addressing Heteroscedasticity:\n",
    "\n",
    "Transforming Variables: Applying transformations like logarithmic or square root transformations to the dependent variable or certain predictor variables can stabilize the variance.\n",
    "Weighted Least Squares (WLS) Regression: Assigning weights to observations inversely proportional to the variance of the residuals can mitigate heteroscedasticity effects.\n",
    "    \n",
    "c. Implementation and Evaluation: \n",
    "    \n",
    "You decide to apply a logarithmic transformation to the employee performance variable and re-run the regression. Additionally, you experiment with WLS regression by assigning appropriate weights to observations.\n",
    "After these remedial actions, you reevaluate the model's performance metrics (such as R-squared, RMSE) and find that they have improved. The model's predictions are now more reliable, and the assumptions of linear regression are better met.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd693d56",
   "metadata": {},
   "source": [
    "4. Detecting Multicollinearity:\n",
    "a. Multicollinearity in Linear Regression:\n",
    "\n",
    "Multicollinearity occurs when predictor variables in a regression model are highly correlated with each other. This correlation can cause issues in the regression analysis, making it difficult to identify the individual effect of each predictor variable on the target. Multicollinearity inflates standard errors, leading to unstable and unreliable coefficient estimates. It does not affect the model's overall fit (R-squared), but it makes it challenging to interpret the importance of each predictor variable.\n",
    "b. Identification Methods:\n",
    "\n",
    "Correlation Matrices: Calculate the correlation coefficients between all pairs of predictor variables. High absolute values (close to 1) indicate strong correlations.\n",
    "Variance Inflation Factors (VIFs): VIF measures how much the variance of an estimated regression coefficient increases if the predictors are correlated. VIF values greater than 10 indicate a problematic amount of collinearity.\n",
    "c. Findings on Highly Correlated Variables:\n",
    "\n",
    "After calculating correlation coefficients and VIF values for your predictor variables, you find that \"experience\" and \"number of projects completed\" have a high positive correlation coefficient (close to 1). Additionally, the VIF values for both of these variables are above 10, indicating significant multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1389aada",
   "metadata": {},
   "source": [
    "5. Mitigating Multicollinearity:\n",
    "a. Issues Associated with Multicollinearity:\n",
    "\n",
    "Unreliable Coefficients: Multicollinearity makes it challenging to determine the true effect of each predictor variable on the target because the coefficients become unstable and can flip signs erratically.\n",
    "Reduced Interpretability: When predictor variables are highly correlated, it's difficult to isolate and understand the impact of each variable on the target variable. Interpretation becomes ambiguous.\n",
    "b. Strategies for Mitigating Multicollinearity:\n",
    "\n",
    "Feature Selection: Choose a subset of the most relevant predictors based on domain knowledge or statistical techniques like stepwise regression.\n",
    "\n",
    "Regularization Techniques: Regularization methods like Lasso (L1 regularization) and Ridge (L2 regularization) penalize large coefficients, effectively reducing the impact of less important predictors.\n",
    "\n",
    "\n",
    "After evaluating the options, you decide to use Lasso regression, which performs both variable selection and regularization, effectively mitigating multicollinearity and providing a simpler, interpretable model.\n",
    "You apply Lasso regression to the dataset, and it automatically selects the most relevant predictors while penalizing the less relevant ones. After this adjustment, you assess the model's performance using metrics like R-squared, RMSE, and MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5281e38a",
   "metadata": {},
   "source": [
    "6. Model Evaluation:\n",
    "a. Performance Metrics:\n",
    "\n",
    "R-squared (Coefficient of Determination): Indicates the proportion of the variance in the dependent variable (employee performance) that is predictable from the independent variables.\n",
    "\n",
    "MAE (Mean Absolute Error): Represents the average of the absolute errors between predicted and actual performance scores.\n",
    "\n",
    "MSE (Mean Squared Error): Measures the average of the squared errors, giving more weight to large errors.\n",
    "RMSE (Root Mean Squared Error): \n",
    "Square root of MSE, providing an interpretable measure in the same unit as the target variable.\n",
    "\n",
    "b. Interpretation of Coefficients:\n",
    "\n",
    "After addressing heteroscedasticity and multicollinearity, the coefficients of the remaining predictors become more stable and interpretable.\n",
    "For example, if \"experience\" is a significant predictor, a unit increase in experience leads to a certain change in the employee performance score, holding other variables constant. The coefficients indicate the strength and direction of these relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181df6bb",
   "metadata": {},
   "source": [
    "7. Conclusion:\n",
    "a. Impact of Addressing Issues:\n",
    "\n",
    "Identifying and addressing heteroscedasticity and multicollinearity have significantly improved the predictive accuracy and interpretability of the employee performance model.\n",
    "The model's performance metrics have likely shown enhancements, with a higher R-squared value indicating a better fit to the data. The MAE, MSE, and RMSE values have likely reduced, indicating smaller prediction errors.\n",
    "b. Recommendations for Future Model Development:\n",
    "\n",
    "Feature Engineering: Consider exploring additional relevant features that could enhance the model's predictive power.\n",
    "Continuous Monitoring: Regularly check for new data patterns and reevaluate the model's performance as the dataset evolves.\n",
    "External Factors: Incorporate external factors like market trends or company policies that might influence employee performance.\n",
    "Advanced Techniques: Explore advanced machine learning algorithms beyond linear regression, like decision trees, random forests, or neural networks, to capture complex relationships."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
