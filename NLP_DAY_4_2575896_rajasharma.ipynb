{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation"
      ],
      "metadata": {
        "id": "k9C9ZgQaT6Ru"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Get a list of valid words in English\n",
        "nltk.download('words')\n",
        "valid_words = set(words.words()[:20000])  # Using the first 20,000 entries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78HyeiwKT7w-",
        "outputId": "8460d57e-1651-4f25-8ca2-e7d9340ece11"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Display the first 20 words\n",
        "print(\"First 20 words in the list:\", list(valid_words)[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mo4NdmGeT_J-",
        "outputId": "b20bb5d5-bbd8-4415-8478-5bf7d67c8100"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 words in the list: ['avania', 'aly', 'autocratrix', 'acaudal', 'another', 'adolescently', 'Attic', 'Anglicization', 'airfoil', 'afterhope', 'altared', 'Anthophila', 'absconded', 'apocrenic', 'arcuale', 'agglutinogen', 'acephalia', 'aln', 'Ainu', 'Agdistis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Normalize  casing for all terms\n",
        "valid_words_normalized = set(word.lower() for word in valid_words)"
      ],
      "metadata": {
        "id": "FKKCOEeEUE8v"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Create a unique list after normalizing\n",
        "print(\"Unique list of valid words (after normalization):\", list(valid_words_normalized)[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTij0ZakUIdQ",
        "outputId": "206cd1b1-78a5-4031-d8ab-f5fa23d18c79"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique list of valid words (after normalization): ['arabophil', 'avania', 'aly', 'autocratrix', 'antiaris', 'acaudal', 'another', 'alejandro', 'adolescently', 'airfoil', 'afterhope', 'altared', 'absconded', 'apocrenic', 'arcuale', 'agglutinogen', 'acephalia', 'aln', 'anthracosis', 'aethusa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Create a list of stop words\n",
        "stop_words = set(nltk.corpus.stopwords.words('english') + list(punctuation))\n"
      ],
      "metadata": {
        "id": "es2R5uEIUMge"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Define a function to get the correct term\n",
        "def get_correct_term(term):\n",
        "    # Calculate edit distance with each term in the first 20,000 entries\n",
        "    edit_distances = {valid_word: nltk.edit_distance(term, valid_word) for valid_word in list(valid_words_normalized)[:20000]}\n",
        "\n",
        "    # Sort the dictionary by edit distance in ascending order\n",
        "    sorted_distances = sorted(edit_distances.items(), key=lambda x: x[1])\n",
        "\n",
        "    # Return the first entry (term with minimum edit distance)\n",
        "    return sorted_distances[0][0]"
      ],
      "metadata": {
        "id": "ilM40xOIUQrm"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Make a set from the list of valid words for faster lookup\n",
        "valid_words_set = set(valid_words_normalized)"
      ],
      "metadata": {
        "id": "thTXmS8SUVQv"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Define a function for spelling correction\n",
        "def correct_spelling(sentence):\n",
        "    # Tokenize the sentence after making all terms lowercase\n",
        "    tokenized_sentence = word_tokenize(sentence.lower())\n",
        "\n",
        "    # Check and correct each term\n",
        "    corrected_sentence = [word if word in valid_words_set else get_correct_term(word) for word in tokenized_sentence]\n",
        "\n",
        "    # Return the joined string as output\n",
        "    return ' '.join(corrected_sentence)"
      ],
      "metadata": {
        "id": "A1-mrjJ4UYwn"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Test the function\n",
        "input_sentence = \"The new abacos is great\"\n",
        "output_sentence = correct_spelling(input_sentence)\n",
        "print(\"Corrected Sentence:\", output_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GcbBHzXUcCH",
        "outputId": "4ba66e0a-d483-455e-89e7-8fa2582b4ef4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected Sentence: ahem anew abacus as area\n"
          ]
        }
      ]
    }
  ]
}