{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d201f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk \n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c17df1e",
   "metadata": {},
   "source": [
    "# BOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d5ce333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aba decides against community broadcasting licence', 'act fire witnesses must be aware of defamation', 'a g calls for infrastructure protection summit', 'air nz staff in aust strike for pay rise', 'air nz strike to affect australian travellers']\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('abcnews-date-text.csv') \n",
    "\n",
    "headlines = data['headline_text']\n",
    "\n",
    "dataset = []\n",
    "for headline in headlines:\n",
    "    headline = headline.lower()  # Convert to lowercase\n",
    "    headline = re.sub(r'\\W', ' ', headline)  # Remove non-word characters\n",
    "    headline = re.sub(r'\\s+', ' ', headline)  # Remove extra whitespaces\n",
    "    tokens = nltk.word_tokenize(headline)  # Tokenize the headline\n",
    "    dataset.append(' '.join(tokens))  # Append tokenized headline to the dataset\n",
    "\n",
    "print(dataset[:5])  # Print the first 5 tokenized headlines for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29460f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0]\n",
      " [0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow_representation = vectorizer.fit_transform(dataset[:5])  # Use the processed headlines\n",
    "\n",
    "bow_array = bow_representation.toarray()\n",
    "\n",
    "print(bow_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230004e4",
   "metadata": {},
   "source": [
    "# N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16f87bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: [['aba', 'decides', 'against', 'community', 'broadcasting', 'licence'], ['act', 'fire', 'witnesses', 'must', 'be', 'aware', 'of', 'defamation'], ['a', 'g', 'calls', 'for', 'infrastructure', 'protection', 'summit'], ['air', 'nz', 'staff', 'in', 'aust', 'strike', 'for', 'pay', 'rise'], ['air', 'nz', 'strike', 'to', 'affect', 'australian', 'travellers']]\n",
      "\n",
      "Generated 2-grams:\n",
      "(['aba', 'decides', 'against', 'community', 'broadcasting', 'licence'], ['act', 'fire', 'witnesses', 'must', 'be', 'aware', 'of', 'defamation'])\n",
      "(['act', 'fire', 'witnesses', 'must', 'be', 'aware', 'of', 'defamation'], ['a', 'g', 'calls', 'for', 'infrastructure', 'protection', 'summit'])\n",
      "(['a', 'g', 'calls', 'for', 'infrastructure', 'protection', 'summit'], ['air', 'nz', 'staff', 'in', 'aust', 'strike', 'for', 'pay', 'rise'])\n",
      "(['air', 'nz', 'staff', 'in', 'aust', 'strike', 'for', 'pay', 'rise'], ['air', 'nz', 'strike', 'to', 'affect', 'australian', 'travellers'])\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('abcnews-date-text.csv')\n",
    "\n",
    "headlines = data['headline_text']\n",
    "\n",
    "dataset = []\n",
    "for headline in headlines:\n",
    "    headline = headline.lower()  # Convert to lowercase\n",
    "    headline = re.sub(r'\\W', ' ', headline)  # Remove non-word characters\n",
    "    headline = re.sub(r'\\s+', ' ', headline)  # Remove extra whitespaces\n",
    "    tokens = nltk.word_tokenize(headline)  # Tokenize the headline\n",
    "    dataset.append(tokens)  # Append tokenized headline to the dataset\n",
    "\n",
    "n = 2  # Set the value of n for n-grams (bigrams in this case)\n",
    "bigrams = list(ngrams(dataset[:5], n))  # Generate n-grams from the first 5 tokenized headlines\n",
    "\n",
    "print(f\"Original text: {dataset[:5]}\")\n",
    "print()\n",
    "print(f\"Generated {n}-grams:\")\n",
    "for gram in bigrams:\n",
    "    print(gram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc07fd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: [['aba', 'decides', 'against', 'community', 'broadcasting', 'licence'], ['act', 'fire', 'witnesses', 'must', 'be', 'aware', 'of', 'defamation'], ['a', 'g', 'calls', 'for', 'infrastructure', 'protection', 'summit'], ['air', 'nz', 'staff', 'in', 'aust', 'strike', 'for', 'pay', 'rise'], ['air', 'nz', 'strike', 'to', 'affect', 'australian', 'travellers'], ['ambitious', 'olsson', 'wins', 'triple', 'jump']]\n",
      "\n",
      "Generated 3-grams:\n",
      "(['aba', 'decides', 'against', 'community', 'broadcasting', 'licence'], ['act', 'fire', 'witnesses', 'must', 'be', 'aware', 'of', 'defamation'], ['a', 'g', 'calls', 'for', 'infrastructure', 'protection', 'summit'])\n",
      "(['act', 'fire', 'witnesses', 'must', 'be', 'aware', 'of', 'defamation'], ['a', 'g', 'calls', 'for', 'infrastructure', 'protection', 'summit'], ['air', 'nz', 'staff', 'in', 'aust', 'strike', 'for', 'pay', 'rise'])\n",
      "(['a', 'g', 'calls', 'for', 'infrastructure', 'protection', 'summit'], ['air', 'nz', 'staff', 'in', 'aust', 'strike', 'for', 'pay', 'rise'], ['air', 'nz', 'strike', 'to', 'affect', 'australian', 'travellers'])\n",
      "(['air', 'nz', 'staff', 'in', 'aust', 'strike', 'for', 'pay', 'rise'], ['air', 'nz', 'strike', 'to', 'affect', 'australian', 'travellers'], ['ambitious', 'olsson', 'wins', 'triple', 'jump'])\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams as nltk_ngrams\n",
    "\n",
    "data = pd.read_csv('abcnews-date-text.csv')\n",
    "\n",
    "headlines = data['headline_text']\n",
    "\n",
    "dataset = []\n",
    "for headline in headlines:\n",
    "    headline = headline.lower()  # Convert to lowercase\n",
    "    headline = re.sub(r'\\W', ' ', headline)  # Remove non-word characters\n",
    "    headline = re.sub(r'\\s+', ' ', headline)  # Remove extra whitespaces\n",
    "    tokens = nltk.word_tokenize(headline)  # Tokenize the headline\n",
    "    dataset.append(tokens)  # Append tokenized headline to the dataset\n",
    "\n",
    "n = 3  # Set the value of n for n-grams (trigrams in this case)\n",
    "generated_ngrams = list(nltk_ngrams(dataset[:6], n))  # Generate n-grams from the first 6 tokenized headlines\n",
    "\n",
    "print(f\"Original text: {dataset[:6]}\")\n",
    "print()\n",
    "print(f\"Generated {n}-grams:\")\n",
    "for gram in generated_ngrams:\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd1c98",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb49a35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names(TF-IDF):\n",
      " ['headline_text' 'publish_date']\n",
      "\n",
      "TF-IDF Matrix:\n",
      " [[0.40824829 0.         0.         0.40824829 0.         0.\n",
      "  0.         0.         0.         0.         0.40824829 0.\n",
      "  0.40824829 0.40824829 0.         0.         0.         0.\n",
      "  0.         0.         0.40824829 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.35355339 0.         0.         0.         0.\n",
      "  0.         0.         0.35355339 0.35355339 0.         0.\n",
      "  0.         0.         0.35355339 0.35355339 0.         0.\n",
      "  0.         0.         0.         0.35355339 0.         0.35355339\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.35355339]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.46262479\n",
      "  0.         0.         0.         0.         0.37935895 0.\n",
      "  0.46262479 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.46262479 0.         0.         0.\n",
      "  0.46262479 0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.29571061 0.\n",
      "  0.3606164  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.29571061 0.3606164\n",
      "  0.         0.         0.         0.         0.29571061 0.\n",
      "  0.         0.3606164  0.         0.3606164  0.3606164  0.29571061\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.40766201 0.         0.33428868 0.\n",
      "  0.         0.40766201 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.33428868 0.\n",
      "  0.         0.         0.         0.         0.         0.33428868\n",
      "  0.         0.40766201 0.40766201 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.4472136\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.4472136  0.         0.         0.         0.\n",
      "  0.4472136  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.4472136  0.4472136  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data = pd.read_csv('abcnews-date-text.csv')\n",
    "\n",
    "headlines = data['headline_text']\n",
    "\n",
    "dataset = []\n",
    "for headline in headlines:\n",
    "    headline = headline.lower()  # Convert to lowercase\n",
    "    headline = re.sub(r'\\W', ' ', headline)  # Remove non-word characters\n",
    "    headline = re.sub(r'\\s+', ' ', headline)  # Remove extra whitespaces\n",
    "    #tokens = nltk.word_tokenize(headline)  # Tokenize the headline\n",
    "    dataset.append(headline)\n",
    "    \n",
    "tfidf = TfidfVectorizer()\n",
    "x_tfidf = tfidf.fit_transform(dataset[:6])  # Use the preprocessed headlines\n",
    "\n",
    "feature_names_tfidf = tfidf.get_feature_names_out()\n",
    "\n",
    "print('Feature Names(TF-IDF):\\n',features_names_tfidf)\n",
    "\n",
    "print()\n",
    "\n",
    "print('TF-IDF Matrix:\\n',x_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8780bc",
   "metadata": {},
   "source": [
    "# One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "640a8d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names (One-Hot Encoding):\n",
      " ['aba' 'act' 'affect' 'against' 'air' 'ambitious' 'aust' 'australian'\n",
      " 'aware' 'be' 'broadcasting' 'calls' 'community' 'decides' 'defamation'\n",
      " 'fire' 'for' 'in' 'infrastructure' 'jump' 'licence' 'must' 'nz' 'of'\n",
      " 'olsson' 'pay' 'protection' 'rise' 'staff' 'strike' 'summit' 'to'\n",
      " 'travellers' 'triple' 'wins' 'witnesses']\n",
      "\n",
      "One-Hot Encoded Matrix:\n",
      " [[1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "data = pd.read_csv('abcnews-date-text.csv')\n",
    "\n",
    "headlines = data['headline_text']\n",
    "\n",
    "processed_headlines = []\n",
    "for headline in headlines:\n",
    "    headline = headline.lower()  # Convert to lowercase\n",
    "    headline = re.sub(r'\\W', ' ', headline)  # Remove non-word characters\n",
    "    headline = re.sub(r'\\s+', ' ', headline)  # Remove extra whitespaces\n",
    "    processed_headlines.append(headline)\n",
    "\n",
    "# Using CountVectorizer for one-hot encoding\n",
    "count_vectorizer = CountVectorizer(binary=True)\n",
    "x_one_hot = count_vectorizer.fit_transform(processed_headlines[:6])  # Use the processed headlines\n",
    "\n",
    "feature_names_one_hot = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "print('Feature Names (One-Hot Encoding):\\n', feature_names_one_hot)\n",
    "print()\n",
    "print('One-Hot Encoded Matrix:\\n', x_one_hot.toarray())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
